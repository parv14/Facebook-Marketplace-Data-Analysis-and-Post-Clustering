# -*- coding: utf-8 -*-
"""Facebook_Live_Sellers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-tIYTkfXnI7l_iNjBItu0VH4aI6FC56k
"""



import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df=pd.read_csv('Facebook_Marketplace_data.csv')

df['status_published'] = pd.to_datetime(df['status_published'])

# Extract time-related features
df['hour'] = df['status_published'].dt.hour
df['day_of_week'] = df['status_published'].dt.dayofweek
df['month'] = df['status_published'].dt.month

# Calculate average reactions by hour
hourly_reactions = df.groupby('hour')['num_reactions'].mean().reset_index()

# Plot reactions by hour
plt.figure(figsize=(12, 6))
sns.lineplot(x='hour', y='num_reactions', data=hourly_reactions)
plt.title('Average Number of Reactions by Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('Average Number of Reactions')
plt.xticks(range(0, 24))
plt.show()

# Calculate and plot correlations
time_features = ['hour', 'day_of_week', 'month', 'num_reactions']
correlation_matrix = df[time_features].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)
plt.title('Correlation between Time Features and Number of Reactions')
plt.show()

correlation_matrix = df[['num_reactions', 'num_comments', 'num_shares']].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation between Reactions, Comments, and Shares')
plt.show()

print(correlation_matrix)

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder

columns_to_use = ['status_type', 'num_reactions', 'num_comments', 'num_shares',
                  'num_likes', 'num_loves', 'num_wows', 'num_hahas', 'num_sads', 'num_angrys']
df_cluster = df[columns_to_use]

# One-hot encode the 'status_type' column
onehot = OneHotEncoder(sparse_output=False)
status_type_encoded = onehot.fit_transform(df_cluster[['status_type']])

# Get feature names for one-hot encoded columns
status_type_columns = onehot.get_feature_names_out(['status_type'])

# Create a new dataframe with encoded features
df_encoded = pd.concat([df_cluster.drop('status_type', axis=1).reset_index(drop=True),
                        pd.DataFrame(status_type_encoded, columns=status_type_columns)], axis=1)

# Scale the features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_encoded)

# Train K-Means model (assuming 5 clusters for now)
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(df_scaled)

# Add cluster labels to the original dataframe
df['cluster'] = clusters

print("Clustering completed. Cluster labels added to the dataframe.")
print(df['cluster'].value_counts())

from sklearn.cluster import KMeans

inertias = []
k_range = range(1, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_scaled)
    inertias.append(kmeans.inertia_)

plt.plot(k_range, inertias, marker='o')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()

post_type_counts = df['status_type'].value_counts()
print(post_type_counts)

plt.figure(figsize=(10, 6))
post_type_counts.plot(kind='bar')
plt.title('Count of Different Post Types')
plt.xlabel('Post Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

avg_metrics = df.groupby('status_type')[['num_reactions', 'num_comments', 'num_shares']].mean()
print(avg_metrics)

avg_metrics.plot(kind='bar', figsize=(12, 6))
plt.title('Average Engagement Metrics by Post Type')
plt.xlabel('Post Type')
plt.ylabel('Average Value')
plt.legend(title='Metric')
plt.xticks(rotation=45)
plt.show()